{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Big 3","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"PStxwrJmBKGG","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!ls drive/'My Drive'/data/"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lOwr_OAMDHnL","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.contrib import rnn"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i6IYf2K7DmSs","colab_type":"code","colab":{}},"cell_type":"code","source":["np.random.seed(777)\n","tf.set_random_seed(777)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c6KERHooDtBD","colab_type":"code","colab":{}},"cell_type":"code","source":["# reno cubic bbr bic westwood htcp vegas veno scalable highspeed\n","\n","# パラメーター\n","TCP_LIST = [\"reno\", \"cubic\", \"bbr\"]\n","TCP_NAME_LIST = [\"Reno\", \"CUBIC\", \"BBR\"]\n","# TCP_LIST = [\"reno\", \"cubic\", \"bbr\", \"bic\"]\n","# TCP_NAME_LIST = [\"Reno\", \"CUBIC\", \"BBR\", \"BIC\"]\n","# TCP_LIST = [\"reno\", \"cubic\", \"bbr\", \"westwood\"]\n","# TCP_NAME_LIST = [\"Reno\", \"CUBIC\", \"BBR\", \"Westwood\"],\n","# TCP_LIST = [\"reno\", \"cubic\", \"bbr\", \"htcp\"],\n","# TCP_NAME_LIST = [\"Reno\", \"CUBIC\", \"BBR\", \"HTCP\"]\n","# TCP_LIST = [\"reno\", \"cubic\", \"bbr\", \"vegas\"],\n","# TCP_NAME_LIST = [\"Reno\", \"CUBIC\", \"BBR\", \"Vegas\"]\n","# TCP_LIST = [\"reno\", \"cubic\", \"bbr\", \"veno\"],\n","# TCP_NAME_LIST = [\"Reno\", \"CUBIC\", \"BBR\", \"Veno\"]\n","# TCP_LIST = [\"reno\", \"cubic\", \"bbr\", \"scalable\"],\n","# TCP_NAME_LIST = [\"Reno\", \"CUBIC\", \"BBR\", \"Scalable\"]\n","# TCP_LIST = [\"reno\", \"cubic\", \"bbr\", \"highspeed\"]\n","# TCP_NAME_LIST = [\"Reno\", \"CUBIC\", \"BBR\", \"HighSpeed\"]\n","\n","LABEL_LIST = [i for i in range(len(TCP_LIST))]\n","N_CLASSES = len(TCP_LIST) # クラス数\n","N_INPUTS = 1              # 1ステップに入力されるデータ数\n","N_STEPS = 2000            # 学習ステップ数\n","SEQ_LEN = 128             # 系列長\n","N_NODES = 512             # ノード数\n","N_DATA = 400              # 各クラスの学習用データ数\n","N_TEST = 400              # テスト用データ数\n","BATCH_SIZE = 128          # バッチサイズ\n","LOSS_DIFF = 0.2           # model 復元する loss の閾値補正\n","LOSS_PROD = 2             # model 復元する loss の閾値倍率\n","TEST_FREQ = 10            # テストの実行頻度"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p_Q8KWN6DwL9","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_dataframe(i, seq_len, n_data, path):\n","    df = pd.read_csv(path).dropna()[:n_data]    # 前から\n","    label = np.full([n_data, 1], i)\n","    return np.hstack((df.values, label)).reshape(-1, seq_len + 1, 1)\n","\n","\n","def gen_dataset(tcp_list, seq_len, n_data, path_str):\n","    dfs = [read_dataframe(i, seq_len, n_data, path_str.format(tcp_list[i])) for i in range(len(tcp_list))]\n","    dataset = np.concatenate(dfs)\n","    np.random.shuffle(dataset)\n","    x_ = dataset[:, : seq_len]\n","    t_ = dataset[:, seq_len].reshape(-1)\n","    return x_, t_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E7wmRc76D6Px","colab_type":"code","colab":{}},"cell_type":"code","source":["# モデルの構築\n","x = tf.placeholder(tf.float32, [None, SEQ_LEN, N_INPUTS])  # 入力データ\n","t = tf.placeholder(tf.int32, [None])  # 教師データ\n","t_on_hot = tf.one_hot(t, depth=N_CLASSES, dtype=tf.float32)  # 1-of-Kベクトル\n","cell = rnn.LSTMCell(num_units=N_NODES, activation=tf.nn.tanh)  # 中間層のセル\n","# RNNに入力およびセル設定する\n","outputs, states = tf.nn.dynamic_rnn(\n","    cell=cell, inputs=x, dtype=tf.float32, time_major=False)\n","# [ミニバッチサイズ,系列長,出力数]→[系列長,ミニバッチサイズ,出力数]\n","outputs = tf.transpose(outputs, perm=[1, 0, 2])\n","\n","w = tf.Variable(tf.random_normal([N_NODES, N_CLASSES], stddev=0.01))\n","b = tf.Variable(tf.zeros([N_CLASSES]))\n","logits = tf.matmul(outputs[-1], w) + b  # 出力層\n","y = tf.nn.softmax(logits)  # ソフトマックス\n","\n","cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n","    labels=t_on_hot, logits=logits)\n","loss = tf.reduce_mean(cross_entropy)  # 誤差関数\n","train_step = tf.train.AdamOptimizer(\n","    learning_rate=2e-4).minimize(loss)  # 学習アルゴリズム\n","\n","correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t_on_hot, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  # 精度"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xHQlUi2oEnIz","colab_type":"code","colab":{}},"cell_type":"code","source":["train_dataset_path_str = 'drive/My Drive/data/train/{}.csv'\n","test_dataset_path_str = 'drive/My Drive/data/test/{}.csv'\n","\n","\n","x_train, t_train = gen_dataset(TCP_LIST, SEQ_LEN, N_DATA, train_dataset_path_str)\n","x_test, t_test = gen_dataset(TCP_LIST, SEQ_LEN, N_TEST, test_dataset_path_str)\n","\n","\n","# 学習の実行\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","\n","saver = tf.train.Saver()\n","loss_, loss_min,loss_test_, loss_test_min = [255] * 4 # Huge\n","epoch_min = 0\n","\n","\n","results = []\n","\n","\n","filename = str('-'.join(TCP_LIST))\n","\n","modelpath = 'drive/My Drive/data/model/{}'.format(filename)\n","  \n","i = 0\n","for _ in range(N_STEPS):\n","    cycle = int(N_DATA * 3 / BATCH_SIZE)\n","    begin = int(BATCH_SIZE * (i % cycle))\n","    end = begin + BATCH_SIZE\n","    x_batch, t_batch = x_train[begin:end], t_train[begin:end]\n","    sess.run(train_step, feed_dict={x: x_batch, t: t_batch})\n","    i += 1\n","    if i % TEST_FREQ == 0:\n","        best = \"[   ]\"\n","\n","        loss_, acc_ = sess.run([loss, accuracy], feed_dict={x: x_batch, t: t_batch})\n","        loss_test_, acc_test_ = sess.run([loss, accuracy], feed_dict={x: x_test, t: t_test})\n","\n","        # Best Update\n","        if loss_min > loss_ and loss_test_min > loss_test_:\n","            epoch_min = i\n","            loss_min = loss_\n","            loss_test_min = loss_test_\n","            best = \"[ B ]\"\n","            saver.save(sess, modelpath)\n","\n","\n","        if (loss_min + LOSS_DIFF < loss_ and loss_test_min + LOSS_DIFF < loss_) or \\\n","           (loss_min * LOSS_PROD < loss_ and loss_test_min * LOSS_PROD < loss_test_):\n","            saver.restore(sess, modelpath)\n","            print(\"Restore Occured! Epoch:{}\".format(epoch_min))\n","            loss_, acc_ = sess.run([loss, accuracy], feed_dict={x: x_batch, t: t_batch})\n","            loss_test_, acc_test_ = sess.run([loss, accuracy], feed_dict={x: x_test, t: t_test})\n","\n","        print(best + \"{:>5} [TRAIN] loss : {:f}, acc : {:f}  [TEST] loss : {:f}, acc : {:f}\".format(i, loss_, acc_, loss_test_, acc_test_))\n","\n","        results.append([i, acc_, acc_test_])\n","\n","saver.restore(sess, modelpath)\n","pred = sess.run(y, feed_dict={x: x_test, t: t_test})\n","\n","sess.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xec6EAb9x6GY","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","def plot_result(df, filepath=None, fontsize=20, figsize=(16, 10)):\n","    df.plot(x=df.columns[0], fontsize=fontsize, figsize=figsize)\n","    plt.legend(fontsize=fontsize)\n","    plt.xlabel(df.columns[0], fontsize=fontsize)\n","    plt.yticks(np.arange(0, 1.1, 0.1))\n","    plt.savefig(filepath) if filepath is not None else None\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I9pPYUjn1C8l","colab_type":"code","colab":{}},"cell_type":"code","source":["resultpath = 'drive/My Drive/data/result/process_{}.csv'.format(filename)\n","df = pd.DataFrame(results,  columns=[\"Epoch\", \"Train accuracy\", \"Test Accuracy\"])\n","df.to_csv(resultpath, index=False)\n","plot_result(df.query('Epoch <= 1000'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IuWUbDgSoO1Q","colab_type":"code","colab":{}},"cell_type":"code","source":["# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n","import itertools\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues,\n","                          filepath=None,\n","                          fontsize=20,\n","                          figsize=(16, 10)):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    plt.figure(figsize=figsize)\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title, fontsize=fontsize)\n","    cb = plt.colorbar()\n","    cb.ax.tick_params(labelsize=fontsize)\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, fontsize=fontsize, rotation=45)\n","    plt.yticks(tick_marks, classes, fontsize=fontsize)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\",\n","                 fontsize=fontsize)\n","\n","    plt.ylabel('True', fontsize=fontsize)\n","    plt.xlabel('Predicted', fontsize=fontsize)\n","    plt.tight_layout()\n","\n","    if filepath is not None:\n","       plt.savefig(filepath)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lLQ3VW64swMQ","colab_type":"code","colab":{}},"cell_type":"code","source":["t_pred = pred.argmax(axis=1)\n","\n","from sklearn.preprocessing import label_binarize\n","b_test = label_binarize(t_test, classes=LABEL_LIST)\n","b_pred = label_binarize(t_pred, classes=LABEL_LIST)\n","\n","# from sklearn.metrics import accuracy_score\n","# acc = accuracy_score(b_test, b_pred)\n","\n","from sklearn.metrics import confusion_matrix\n","confusion = confusion_matrix(t_test, t_pred, labels=LABEL_LIST)\n","\n","matrixpath = 'drive/My Drive/data/result/confusion_matrix_{}.csv'.format(filename)\n","df = pd.DataFrame(confusion, index=TCP_NAME_LIST, columns=TCP_NAME_LIST)\n","df.to_csv(matrixpath)\n","plot_confusion_matrix(confusion,\n","                      classes=TCP_NAME_LIST,\n","                      normalize=False,\n","                      title='Confusion Matrix')"],"execution_count":0,"outputs":[]}]}